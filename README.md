# CS 4641 Group 9: Project Proposal
*Machine learning project on mapping facial expressions to Apple emojis*

## Introduction/background 

Recognition of emotions through software has been an area of research that has been rapidly growing over the past couple of years. With facial detection evolving and being integrated into everyday tasks (i.e., unlocking your phone, using digital pay), it is time that facial recognition moves further but now in detecting emotions. Some of the past work that has already been done involves automatic detection of emotions in human photographs by Kosti (Kosti, 2019) and automatic detection using dual-channel expression learning algorithm by Song (Song, 2021) to improve forms of interpersonal communication through emotion recognition. Inspired by this, we wanted to replicate emotion detection through facial expressions and map them to Apple emojis as emojis are a widely used form of communication online. Our preferred dataset is the Kosti dataset because it is a large dataset of about 156K labeled images. While we are waiting to be approved to use this dataset, we have a backup just in case. This will allow us to effectively identify human emotions through facial recognition with minimal error. 

Kosti Dataset: http://sunai.uoc.edu/emotic/index.html
Backup Dataset: https://research.google/tools/datasets/google-facial-expression/
 

## Problem definition 

If we want computers to be genuinely intelligent, adapt to us, and interact naturally with us, they will need the ability to recognize and express emotions (Picard, 2021). What seems like a fun project can impact several aspects of our lives. By mapping facial expressions to emojis, we will create a technology that benefits many institutions. Some industries include healthcare, gaming, security, and marketing. For example, a marketing and PR firm can use emotion detection to see how people react to a particular marketing campaign. The healthcare industry can use this technology to detect human feelings during treatment. We need to bridge the gap between technology and humans, and emotion detection is a great way to do that. 

 

## Methods 

While emotions depend primarily on the context surrounding facial expressions or body language (Kosti, 2019), we aim to create a system that recognizes apparent emotions from visual information. We will achieve this by following these three steps: facial detection, feature extraction, and feature classification (Song, 2021).  

The dataset we will be using for this project comprises thousands of images showing different emotions displayed by thousands of people.   

We plan to use feature extraction and pattern recognition algorithms to find features common to each emotion (e.g., raised brows to show surprise). We will then train our algorithm to recognize these facial expressions as shown on end-users as well as emojis with little to no margin of error using supervised learning (classification). 

 

## Potential results and discussion 

The ideal result for this project would be to create a system that accurately deciphers and classifies the emotions of people based on their facial expressions, with the ability to map those expressions to Apple emojis.  

This could be useful in various instances, including online learning to enable automatic tutors to provide better feedback for students according to their level of motivation or frustration (Kosti, 2019), and with online texting platforms to make communication more accessible to people with vision impairment. 

 

## References 

- Kosti, R. (2019). Context Based Emotion Recognition using EMOTIC Dataset. Retrieved February 23, 2022, from http://sunai.uoc.edu/emotic/pdf/emotic_pami2019.pdf 

- Kosti, R. (2017). Emotion Recognition in Context. Retrieved February 23, 2022, from http://sunai.uoc.edu/emotic/pdf/emotic_cvpr2017.pdf 

- Islam, L. (2021). Emotion Classification and Emoji Mapping using Convolutional Neural Network. Retrieved February 23, 2022, from https://www.irjet.net/archives/V8/i5/IRJET-V8I5275.pdf 

- Song, Z. (2021, September 27). Facial Expression Emotion Recognition Model Integrating Philosophy and Machine Learning Theory. Retrieved February 23, 2022, from https://www.frontiersin.org/articles/10.3389/fpsyg.2021.759485/full 

- Picard, R. W. (2021, August 24). When Ai recognizes emotion, with Rosalind Picard. MIT Media Lab. Retrieved February 22, 2022, from https://www.media.mit.edu/articles/when-ai-recognizes-emotion-with-rosalind-picard/ 


## Timeline 
Attached below is our anticipated timeline for completing this project. You may click [here](https://docs.google.com/spreadsheets/d/1gdtmGyuqDqsXoawf1rPn08ourgeTl3u5ylTovp2m1GM/edit?usp=sharing) to get a closer look as well.
![timeline](timeline.png)
